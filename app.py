# app.py
# GUI CustomTkinter + YOLOv8 + FaceNet
# - Live camera trong GUI
# - T·ª± ƒë·ªông ch·ª•p 1 ·∫£nh m·ªói N gi√¢y
# - Auto-train sau khi ch·ª•p xong
# - ‚≠ê N√ÇNG C·∫§P: D√πng Cosine Distance ƒë·ªÉ so s√°nh embeddings ‚≠ê
# - ‚≠ê N√ÇNG C·∫§P VIP: Ch·∫ø ƒë·ªô T·ªëi (Dark Mode) t·ª± ƒë·ªông khi kh√¥ng c√≥ ng∆∞·ªùi ‚≠ê

import os
import time
import threading
import pickle
import shutil
from datetime import datetime

import customtkinter as ctk
from tkinter import messagebox, simpledialog
from PIL import Image, ImageTk, ImageDraw

import cv2
import numpy as np
import pandas as pd
import torch
from scipy.spatial.distance import cosine # ‚≠ê TH√äM SCIPY.COSINE ‚≠ê

from ultralytics import YOLO
from facenet_pytorch import MTCNN, InceptionResnetV1
# MediaPipe aligner (local)
from aligner import align_face_mediapipe, center_crop_resize

# ---------------- CONFIG ----------------
YOLO_WEIGHTS = "C:/Users/admin/Downloads/Face-recognition-using-YoloV8-and-Facenet-main/Face-recognition-using-YoloV8-and-Facenet-main/detection/weights/best02m.pt"
KNOWN_FACES_DIR = "known_faces"         # m·ªói ng∆∞·ªùi m·ªôt th∆∞ m·ª•c
EMBEDDINGS_FILE = "known_embeddings.pkl"
ATTENDANCE_FILE = "attendance.csv"
CAPTURE_LOG_DIR = "captured_logs"

LAST_LOG_TIME = {} # Th·ªùi gian l·∫ßn cu·ªëi ch·∫•m c√¥ng c·ªßa m·ªói ng∆∞·ªùi
COOLDOWN_SECONDS = 10 # Th·ªùi gian ch·ªù gi·ªØa 2 l·∫ßn ch·∫•m c√¥ng li√™n ti·∫øp (c√≥ th·ªÉ thay ƒë·ªïi)

# Default auto-capture settings (c√≥ th·ªÉ thay)
AUTO_CAPTURE_INTERVAL = 3.0    # gi√¢y gi·ªØa 2 l·∫ßn ch·ª•p
AUTO_CAPTURE_COUNT = 10        # s·ªë ·∫£nh c·∫ßn ch·ª•p cho 1 ng∆∞·ªùi

# ‚≠ê C·∫§U H√åNH VIP: DYNAMIC ACTIVATION ‚≠ê
DARK_MODE_TIMEOUT = 8 # S·ªë gi√¢y kh√¥ng c√≥ ng∆∞·ªùi s·∫Ω chuy·ªÉn sang n·ªÅn ƒëen/t·ªëi
last_detection_time = time.time() # Th·ªùi ƒëi·ªÉm ph√°t hi·ªán khu√¥n m·∫∑t g·∫ßn nh·∫•t

# Ng∆∞·ª°ng Cosine Distance cho ƒë·ªô ch√≠nh x√°c cao
EMBEDDING_THRESHOLD = 0.20

# ---------------- Make dirs ----------------
os.makedirs(KNOWN_FACES_DIR, exist_ok=True)
os.makedirs(CAPTURE_LOG_DIR, exist_ok=True)

# ---------------- Load models (1 l·∫ßn) ----------------
print("[INFO] Loading models (YOLOv8 + MTCNN + FaceNet). Please wait...")
yolo_model = YOLO(YOLO_WEIGHTS) 
#mtcnn = MTCNN(image_size=160, margin=0, keep_all=False)
resnet = InceptionResnetV1(pretrained="vggface2").eval()
print("[INFO] Models loaded.")

# Assuming this setup is done elsewhere:
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') 

resnet = resnet.to(device)

# ---------------- Known embeddings persistence ----------------
def save_known_embeddings(d):
    with open(EMBEDDINGS_FILE, "wb") as f:
        pickle.dump(d, f)

def load_known_embeddings():
    if os.path.exists(EMBEDDINGS_FILE):
        with open(EMBEDDINGS_FILE, "rb") as f:
            return pickle.load(f)
    return {}

known_embeddings = load_known_embeddings()

# ---------------- Notification and Helper Functions ----------------

# Global notification label (defined below in GUI Init)
notification_label = None
lbl_train_status = None

def create_dark_placeholder(width, height):
    """T·∫°o ·∫£nh n·ªÅn ƒëen v·ªõi th√¥ng b√°o placeholder."""
    img = Image.new('RGB', (width, height), color = 'black')
    d = ImageDraw.Draw(img)
    # Th√™m text "H·ªÜ TH·ªêNG ƒêANG NGH·ªà"
    try:
        # T√™n font t√πy thu·ªôc v√†o h·ªá th·ªëng, d√πng font m·∫∑c ƒë·ªãnh n·∫øu Segoe UI kh√¥ng c√≥
        font_size = 30
        try:
            from customtkinter.windows.widgets.core_widget_classes import CTkFont
            font = CTkFont(family="Segoe UI", size=font_size, weight="bold")
        except ImportError:
            font = None
            
        d.text((width/2, height/2), "H·ªÜ TH·ªêNG ƒêANG NGH·ªà", 
               fill=(255, 255, 255), anchor="mm", font=font)
    except Exception:
        d.text((width/2, height/2), "H·ªÜ TH·ªêNG ƒêANG NGH·ªà", 
               fill=(255, 255, 255), anchor="mm")
        
    return img


def show_notification(message, color="green", duration=2000):
    """Hi·ªÉn th·ªã overlay th√¥ng b√°o l·ªõn tr√™n GUI."""
    if notification_label:
        notification_label.configure(
            text=message, 
            fg_color=color
        )
        # S·ª≠ d·ª•ng place ƒë·ªÉ ƒë·∫∑t label ch·ªìng l√™n c√°c ph·∫ßn t·ª≠ kh√°c v√† cƒÉn gi·ªØa
        notification_label.place(relx=0.5, rely=0.5, anchor="center", relwidth=0.6, relheight=0.15)
        
        # L√™n l·ªãch ·∫©n th√¥ng b√°o sau duration milliseconds
        app.after(duration, hide_notification)

def hide_notification():
    """·∫®n overlay th√¥ng b√°o."""
    if notification_label:
        notification_label.place_forget()

# ---------------- Attendance logging ----------------
def load_logged_today():
    today = datetime.now().strftime("%Y-%m-%d")
    s = set()
    if os.path.exists(ATTENDANCE_FILE):
        try:
            df = pd.read_csv(ATTENDANCE_FILE)
            if "Time" in df.columns and "Name" in df.columns:
                df_today = df[df["Time"].astype(str).str.startswith(today)]
                s = set(df_today["Name"].tolist())
        except Exception as e:
            print("[WARN] Could not read attendance.csv:", e)
    return s

attendance_today = load_logged_today()

def log_attendance(name, frame):
    now = datetime.now()
    timestamp = now.strftime("%Y-%m-%d %H:%M:%S")
    today_str = now.strftime("%Y-%m-%d")

    current_logs = []
    if os.path.exists(ATTENDANCE_FILE):
        try:
            df = pd.read_csv(ATTENDANCE_FILE)
            # L·ªçc ra t·∫•t c·∫£ c√°c l·∫ßn ch·∫•m c√¥ng c·ªßa ng∆∞·ªùi n√†y trong ng√†y h√¥m nay
            df_today_person = df[(df["Name"] == name) & (df["Time"].astype(str).str.startswith(today_str))]
            current_logs = df_today_person["Type"].tolist()
        except Exception as e:
            print("[WARN] Could not inspect attendance.csv:", e)

    # Logic m·ªõi: Ch·ªâ cho ph√©p t·ªëi ƒëa 1 l·∫ßn 'V√†o' v√† 1 l·∫ßn 'Ra'
    log_type = None
    if "V√†o" not in current_logs:
        log_type = "V√†o"
    elif "V√†o" in current_logs and "Ra" not in current_logs:
        log_type = "Ra"
    else:
        # ƒê√£ c√≥ c·∫£ 'V√†o' v√† 'Ra'
        print(f"[INFO] {name} ƒë√£ ƒëi·ªÉm danh c·∫£ V√†o v√† Ra h√¥m nay. B·ªè qua.")
        return # Tho√°t, kh√¥ng ch·∫•m c√¥ng n·ªØa

    # N·∫øu x√°c ƒë·ªãnh ƒë∆∞·ª£c lo·∫°i ch·∫•m c√¥ng (V√†o ho·∫∑c Ra)
    if log_type:
        df_new = pd.DataFrame([[name, timestamp, log_type]], columns=["Name", "Time", "Type"])
        df_new.to_csv(ATTENDANCE_FILE, mode='a', index=False, header=not os.path.exists(ATTENDANCE_FILE))

        # Ghi log ·∫£nh (kh√¥ng thay ƒë·ªïi)
        fname = f"{CAPTURE_LOG_DIR}/{name}_{log_type}_{now.strftime('%Y-%m-%d_%H-%M-%S')}.jpg" # Th√™m lo·∫°i ch·∫•m c√¥ng v√†o t√™n file
        os.makedirs(CAPTURE_LOG_DIR, exist_ok=True)
        cv2.imwrite(fname, frame)
        print(f"[INFO] Logged {name} ({log_type}) at {timestamp}, saved image {fname}")
        
        #  HI·ªÇN TH·ªä TH√îNG B√ÅO TR·ª∞C QUAN 
        log_message = f"{log_type.upper()} TH√ÄNH C√îNG: {name}"
        # S·ª≠ d·ª•ng m√†u xanh l√° cho V√†o, m√†u xanh d∆∞∆°ng cho Ra
        color = "green" if log_type == "V√†o" else "blue" # ƒê√£ s·ª≠a l·ªói ch√≠nh t·∫£ string
        # G·ªçi show_notification trong lu·ªìng ch√≠nh c·ªßa GUI
        app.after(0, lambda: show_notification(log_message, color))


# ---------------- Embedding compare ----------------
def compare_embedding(emb):
    """
    So s√°nh embedding s·ª≠ d·ª•ng Cosine Distance.
    """
    if not known_embeddings:
        return "Unknown", float("inf")
    min_dist = float("inf")
    best = "Unknown"
    
    for name, known_emb in known_embeddings.items():
        #  THAY TH·∫æ np.linalg.norm b·∫±ng scipy.spatial.distance.cosine 
        dist = cosine(emb, known_emb) 
        
        if dist < min_dist:
            min_dist = dist
            best = name if dist < EMBEDDING_THRESHOLD else "Unknown"
            
    return best, min_dist

# ---------------- GUI init ----------------
ctk.set_appearance_mode("Light")
ctk.set_default_color_theme("green")

app = ctk.CTk()
app.title("ƒêi·ªÉm danh khu√¥n m·∫∑t") # ƒê·ªïi t√™n cho ph√π h·ª£p
app.geometry("1100x700")

# Frames
top_frame = ctk.CTkFrame(master=app)
top_frame.pack(padx=10, pady=8, fill="both", expand=False)

left_frame = ctk.CTkFrame(master=app)
left_frame.pack(side="left", padx=10, pady=10, fill="both", expand=True)

right_frame = ctk.CTkFrame(master=app, width=300)
right_frame.pack(side="right", padx=10, pady=10, fill="y")

# Title
title = ctk.CTkLabel(master=top_frame, text="üì∑ H·ªá th·ªëng ƒëi·ªÉm danh b·∫±ng khu√¥n m·∫∑t ", font=("Segoe UI", 20, "bold"))
title.pack(pady=6)

# Video label
video_label = ctk.CTkLabel(master=left_frame, text="")
video_label.pack(padx=10, pady=6)

# Log textbox
log_box = ctk.CTkTextbox(master=left_frame, width=760, height=140)
log_box.pack(padx=10, pady=6)

# ‚≠ê TRAINING STATUS LABEL (NEW) ‚≠ê
lbl_train_status = ctk.CTkLabel(master=left_frame, text="", font=("Segoe UI", 14, "italic"), anchor="w")
lbl_train_status.pack(padx=10, pady=(2, 0), fill="x")

# Progress bar
progress = ctk.CTkProgressBar(master=left_frame, width=600)
progress.set(0)
progress.pack(padx=10, pady=4)

# ‚≠ê NOTIFICATION OVERLAY LABEL (NEW) ‚≠ê
notification_label = ctk.CTkLabel(
    master=app,
    text="",
    fg_color="green", # Default color
    text_color="white",
    font=("Segoe UI", 36, "bold"),
    corner_radius=10
)

def log(msg):
    ts = time.strftime("%H:%M:%S")
    log_box.insert("end", f"{ts} - {msg}\n")
    log_box.see("end")
    print(ts, "-", msg)

# Right side controls
lbl_detect = ctk.CTkLabel(master=right_frame, text="üë§ Nh·∫≠n di·ªán: None", font=("Segoe UI", 14, "bold"))
lbl_detect.pack(pady=6)


# MSSV entry
ctk.CTkLabel(master=right_frame, text="MSSV:").pack(pady=(6,2))
entry_mssv = ctk.CTkEntry(master=right_frame, width=220)
entry_mssv.pack(pady=4)

# name entry for register
ctk.CTkLabel(master=right_frame, text="T√™n (ƒëƒÉng k√Ω):").pack(pady=(6,2))
entry_name = ctk.CTkEntry(master=right_frame, width=220)
entry_name.pack(pady=4)

# auto capture settings
ctk.CTkLabel(master=right_frame, text="C√†i ƒë·∫∑t ch·ª•p t·ª± ƒë·ªông:").pack(pady=(8,2))
interval_var = ctk.DoubleVar(value=AUTO_CAPTURE_INTERVAL)
count_var = ctk.IntVar(value=AUTO_CAPTURE_COUNT)
ctk.CTkLabel(master=right_frame, text="Kho·∫£ng (gi√¢y):").pack(pady=(4,0))
interval_entry = ctk.CTkEntry(master=right_frame, textvariable=interval_var, width=120)
interval_entry.pack(pady=2)
ctk.CTkLabel(master=right_frame, text="S·ªë ·∫£nh:").pack(pady=(4,0))
count_entry = ctk.CTkEntry(master=right_frame, textvariable=count_var, width=120)
count_entry.pack(pady=2)

# Buttons
btn_start = ctk.CTkButton(master=right_frame, text="‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu", width=240) # ƒê·ªïi t√™n n√∫t
btn_stop = ctk.CTkButton(master=right_frame, text="‚èπÔ∏è D·ª´ng", width=240)
btn_auto_capture = ctk.CTkButton(master=right_frame, text="üì∏ Ch·ª•p t·ª± ƒë·ªông", width=240)
btn_reload = ctk.CTkButton(master=right_frame, text="üîÅ T·∫£i l·∫°i embeddings", width=240)
btn_stats = ctk.CTkButton(master=right_frame, text="üìä Th·ªëng k√™ h√¥m nay", width=240)
btn_export = ctk.CTkButton(master=right_frame, text="üì• Xu·∫•t Excel", width=240)
btn_declec = ctk.CTkButton(master=right_frame, text="üóëÔ∏è X√≥a khu√¥n m·∫∑t",width=240)
btn_exit = ctk.CTkButton(master=right_frame, text="‚ùå Tho√°t", width=240)

btn_start.pack(pady=6)
btn_stop.pack(pady=6)
btn_auto_capture.pack(pady=6)
btn_reload.pack(pady=6)
btn_stats.pack(pady=6)
btn_export.pack(pady=6)
btn_declec.pack(pady=6)
btn_exit.pack(pady=6)

# ---------------- Camera & state ----------------
video_capture = None
running = False
stop_auto_capture_flag = False
attendance_today = load_logged_today()

# ---------------- Video processing (recognize) ----------------
def process_video(label_widget):
    global video_capture, running, attendance_today, last_detection_time
    import threading, time
    from queue import Queue

    FRAME_SKIP = 0
    FRAME_SIZE = (640, 480)
    running = True
    log("‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu ch·∫•m c√¥ng")

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    yolo_model.to(device)
    resnet.to(device)
    resnet.eval()
    
    # K√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh c·ªßa video label ƒë·ªÉ t·∫°o placeholder
    display_w = 640
    display_h = 640

    frame_queue = Queue(maxsize=2)
    result_queue = Queue(maxsize=1)

    # üé• Thread ƒë·ªçc camera (Lu√¥n ch·∫°y ƒë·ªÉ l·∫•y khung h√¨nh)
    def capture_thread():
        global video_capture
        video_capture = cv2.VideoCapture(0, cv2.CAP_MSMF)
        video_capture.set(cv2.CAP_PROP_FPS, 30)
        if not video_capture.isOpened():
            messagebox.showerror("L·ªói", "Kh√¥ng th·ªÉ m·ªü camera.")
            return

        while running:
            ret, frame = video_capture.read()
            if not ret:
                continue
            frame = cv2.flip(frame, 1) # L·∫≠t ngang
            if not frame_queue.full():
                frame_queue.put(frame)

        video_capture.release()
        log("üé• Camera stopped")

    # üß† Thread x·ª≠ l√Ω YOLO + FaceNet (Lu√¥n ch·∫°y ƒë·ªÉ ki·ªÉm tra s·ª± hi·ªán di·ªán)
    def inference_thread():
        frame_count = 0
        prev_time = time.time()

        while running:
            if frame_queue.empty():
                time.sleep(0.01)
                continue

            frame = frame_queue.get()
            frame_count += 1
            if FRAME_SKIP > 0 and frame_count % FRAME_SKIP != 0:
                continue
            
            # --- B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian YOLO ---
            t0 = time.time()
            try:
                small = cv2.resize(frame, FRAME_SIZE)
                results = yolo_model.predict(small, verbose=False)
            except Exception as e:
                print("[WARN] YOLO error:", e)
                continue
            infer_time = (time.time() - t0) * 1000  # ms

            detections = []
            is_face_detected = False
            
            if len(results) and len(results[0].boxes) > 0:
                is_face_detected = True
                
                for box in results[0].boxes:
                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
                    scale_x = frame.shape[1] / FRAME_SIZE[0]
                    scale_y = frame.shape[0] / FRAME_SIZE[1]
                    x1, y1 = int(x1 * scale_x), int(y1 * scale_y)
                    x2, y2 = int(x2 * scale_x), int(y2 * scale_y)

                    h, w = frame.shape[:2]
                    x1, y1 = max(0, x1), max(0, y1)
                    x2, y2 = min(w, x2), min(h, y2)
                    # Use MediaPipe aligner first (align on the original frame + bbox)
                    try:
                        aligned = align_face_mediapipe(frame, (x1, y1, x2, y2), output_size=160)
                    except Exception as e:
                        aligned = None

                    if aligned is None:
                        # fallback to center-crop + resize (returns RGB)
                        aligned = center_crop_resize(frame, (x1, y1, x2, y2), output_size=160)

                    if aligned is None:
                        continue

                    face_tensor = torch.tensor(aligned.transpose(2, 0, 1)).unsqueeze(0).float() / 255.0
                    face_tensor = face_tensor.to(device)
                    with torch.no_grad():
                        emb = resnet(face_tensor).detach().cpu().numpy().flatten()

                    detections.append((x1, y1, x2, y2, emb))
                    
            # --- T√≠nh FPS ---
            current_time = time.time()
            fps = 1.0 / (current_time - prev_time)
            prev_time = current_time

            # --- C·∫≠p nh·∫≠t th·ªùi gian ph√°t hi·ªán cu·ªëi c√πng n·∫øu c√≥ khu√¥n m·∫∑t ---
            if is_face_detected:
                global last_detection_time
                last_detection_time = current_time
            
            # --- G·ª≠i frame + detect + FPS sang GUI ---
            result_queue.put((frame, detections, fps))

    # üñºÔ∏è Thread c·∫≠p nh·∫≠t GUI (Ch·ªâ c·∫≠p nh·∫≠t khi ƒëang Active)
    def gui_thread():
        frame_count = 0
        while running:
            if result_queue.empty():
                time.sleep(0.01)
                continue

            frame, detections, fps = result_queue.get()
            
            #  LOGIC CHUY·ªÇN CH·∫æ ƒê·ªò 
            time_since_detection = time.time() - last_detection_time
            is_active_mode = time_since_detection < DARK_MODE_TIMEOUT
            
            # X·ª≠ l√Ω ch·∫•m c√¥ng v√† v·∫Ω box (ch·ªâ khi c√≥ detections)
            for (x1, y1, x2, y2, emb) in detections:
                if is_active_mode:
                    name, dist = compare_embedding(emb) # D√πng h√†m compare_embedding m·ªõi (Cosine)
                    color = (0, 255, 0) if name != "Unknown" else (0, 0, 255)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                    cv2.putText(frame, f"{name}", (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
                    cv2.putText(frame, f"{dist:.2f}", (x1, y2 + 25),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 1)

                    if name != "Unknown":
                        # Ki·ªÉm tra cooldown tr∆∞·ªõc khi ch·∫•m c√¥ng
                        if name not in LAST_LOG_TIME or (time.time() - LAST_LOG_TIME[name]) > COOLDOWN_SECONDS:
                            log_attendance(name, frame)
                            LAST_LOG_TIME[name] = time.time() # C·∫≠p nh·∫≠t th·ªùi gian ch·∫•m c√¥ng
                            log(f"‚úÖ ƒê√£ ƒëi·ªÉm danh: {name}")

                    lbl_detect.configure(text=f"üë§ Nh·∫≠n di·ªán: {name}")
                else:
                    # N·∫øu ƒëang trong ch·∫ø ƒë·ªô t·ªëi m√† v·∫´n detect ƒë∆∞·ª£c, kh√¥ng c·∫ßn v·∫Ω box 
                    # v√¨ n√≥ s·∫Ω chuy·ªÉn sang Active ·ªü frame ti·∫øp theo
                    pass 


            # --- HI·ªÇN TH·ªä TRONG ACTIVE MODE ---
            if is_active_mode:
                # Hi·ªÉn th·ªã FPS
                color_fps = (0, 255, 0) if fps > 25 else ((0, 255, 255) if fps > 15 else (0, 0, 255))
                cv2.putText(frame, f"FPS: {fps:.2f}", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, color_fps, 2)
                
                # C·∫≠p nh·∫≠t video label
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_img = Image.fromarray(frame_rgb)
                pil_img = pil_img.resize((display_w, display_h))
                
                # C·∫≠p nh·∫≠t nh√£n nh·∫≠n di·ªán (n·∫øu kh√¥ng c√≥ ai, s·∫Ω gi·ªØ l·∫°i t√™n cu·ªëi c√πng)
                if not detections:
                     lbl_detect.configure(text=f"üë§ Nh·∫≠n di·ªán: None")
                     
            # --- HI·ªÇN TH·ªä TRONG DARK MODE ---
            else: 
                # Chuy·ªÉn sang ·∫£nh n·ªÅn ƒëen
                pil_img = create_dark_placeholder(display_w, display_h)
                lbl_detect.configure(text=f"üë§ Nh·∫≠n di·ªán: Ch·∫ø ƒë·ªô T·ªëi")

            ctk_img = ctk.CTkImage(light_image=pil_img, size=pil_img.size)
            label_widget.configure(image=ctk_img)
            label_widget.image = ctk_img


        log("üñºÔ∏è GUI stopped")

    # üöÄ Kh·ªüi ƒë·ªông ƒëa lu·ªìng
    t1 = threading.Thread(target=capture_thread, daemon=True)
    t2 = threading.Thread(target=inference_thread, daemon=True)
    t3 = threading.Thread(target=gui_thread, daemon=True)
    t1.start()
    t2.start()
    t3.start()

    log(f"‚úÖ Dynamic Mode ƒëang ch·∫°y. Chuy·ªÉn sang T·ªëi sau {DARK_MODE_TIMEOUT} kh√¥ng ph√°t hi·ªán.")
    
import pandas as pd
from datetime import datetime

def export_attendance_to_excel():
    if not os.path.exists(ATTENDANCE_FILE):
        messagebox.showwarning("Th√¥ng b√°o", "Ch∆∞a c√≥ d·ªØ li·ªáu ƒëi·ªÉm danh ƒë·ªÉ xu·∫•t.")
        return
    try:
        df = pd.read_csv(ATTENDANCE_FILE)
        if "Name" not in df.columns or "Time" not in df.columns or "Type" not in df.columns:
            messagebox.showerror("L·ªói", "File ƒëi·ªÉm danh b·ªã thi·∫øu c·ªôt c·∫ßn thi·∫øt (Name, Time, Type).")
            return

        # L·ªçc d·ªØ li·ªáu h√¥m nay
        today_str = datetime.now().strftime("%Y-%m-%d")
        df_today = df[df["Time"].astype(str).str.startswith(today_str)].copy()

        if df_today.empty:
            messagebox.showinfo("Th√¥ng b√°o", "H√¥m nay ch∆∞a c√≥ d·ªØ li·ªáu ƒëi·ªÉm danh.")
            return

        # T√°ch MSSV v√† T√™n t·ª´ c·ªôt Name (ƒë·ªãnh d·∫°ng MSSV_T√™n)
        df_today[["MSSV", "T√™n"]] = df_today["Name"].str.split("_", n=1, expand=True)
        #df_today["T√™n"] = df_today["Name"]
        df_today["Ng√†y"] = pd.to_datetime(df_today["Time"]).dt.strftime("%Y-%m-%d")
        df_today["Gi·ªù"] = pd.to_datetime(df_today["Time"]).dt.strftime("%H:%M:%S")
        df_today["Tr·∫°ng th√°i"] = df_today["Type"]

        # Ch·ªçn c·ªôt c·∫ßn xu·∫•t
        df_export = df_today[["MSSV", "T√™n", "Ng√†y", "Gi·ªù", "Tr·∫°ng th√°i"]]

        out_file = f"attendance_today_{today_str}.xlsx"
        df_export.to_excel(out_file, index=False)
        messagebox.showinfo("Th√†nh c√¥ng", f"ƒê√£ xu·∫•t d·ªØ li·ªáu ƒëi·ªÉm danh h√¥m nay sang {out_file}")
    except Exception as e:
        messagebox.showerror("L·ªói", f"Kh√¥ng th·ªÉ xu·∫•t file Excel:\n{e}")


def start_recognition():
    global running, last_detection_time
    if running:
        return
    # Kh·ªüi t·∫°o th·ªùi gian ph√°t hi·ªán ngay l·∫≠p t·ª©c ƒë·ªÉ b·∫Øt ƒë·∫ßu ·ªü ch·∫ø ƒë·ªô Active
    last_detection_time = time.time() 
    running = True
    threading.Thread(target=process_video, args=(video_label,), daemon=True).start()

def stop_recognition():
    global running
    running = False

# ---------------- Auto-capture action ----------------
def auto_capture_action():
    mssv = entry_mssv.get().strip()
    name = entry_name.get().strip()
    if not mssv or not name:
        messagebox.showwarning("C·∫£nh b√°o", "Vui l√≤ng nh·∫≠p MSSV v√† T√™n.")
        return

    try:
        interval_str = interval_var.get()
        interval = float(interval_str) if interval_str else 1.0  # m·∫∑c ƒë·ªãnh 1 gi√¢y
    except ValueError:
        messagebox.showwarning("C·∫£nh b√°o", "Kho·∫£ng th·ªùi gian kh√¥ng h·ª£p l·ªá!")
        return

    try:
        count_str = count_var.get()
        count = int(count_str) if count_str else 5  # m·∫∑c ƒë·ªãnh 5 ·∫£nh
    except ValueError:
        messagebox.showwarning("C·∫£nh b√°o", "S·ªë ·∫£nh kh√¥ng h·ª£p l·ªá!")
        return

    person_id = f"{mssv}_{name}"

    threading.Thread(
        target=auto_capture_and_train,
        args=(person_id, name, interval, count),
        daemon=True
    ).start()


# ---------------- Auto-capture routine ----------------
# ---------------- Auto-capture n√¢ng c·∫•p ----------------
def auto_capture_and_train(person_id, display_name, interval_sec, total_count):
    """
    Ch·ª•p t·ª± ƒë·ªông nh∆∞ng b·ªè qua ·∫£nh k√©m ch·∫•t l∆∞·ª£ng:
    - M·ªù (blur)
    - Qu√° t·ªëi (dark)
    - Khu√¥n m·∫∑t qu√° nh·ªè
    """
    global stop_auto_capture_flag, video_capture

    person_dir = os.path.join(KNOWN_FACES_DIR, person_id)
    os.makedirs(person_dir, exist_ok=True)

    stop_auto_capture_flag = False
    saved = 0
    last_time = time.time() - interval_sec

    cap = cv2.VideoCapture(0, cv2.CAP_MSMF)
    cap.set(cv2.CAP_PROP_FPS, 30)
    if not cap.isOpened():
        messagebox.showerror("L·ªói", "Kh√¥ng th·ªÉ m·ªü camera ƒë·ªÉ ch·ª•p.")
        return

    log(f"üì∏ B·∫Øt ƒë·∫ßu ch·ª•p t·ª± ƒë·ªông: {total_count} ·∫£nh, m·ªói {interval_sec}s cho '{display_name}'")
    progress.set(0)

    # ------------------ H√ÄM L·ªåC CH·∫§T L∆Ø·ª¢NG ------------------
    def is_blurry(img, threshold=100.0):
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        return cv2.Laplacian(gray, cv2.CV_64F).var() < threshold

    def is_too_dark(img, threshold=50):
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        return np.mean(gray) < threshold

    def is_face_too_small(box, min_width=50, min_height=50):
        x1, y1, x2, y2 = box
        w, h = x2 - x1, y2 - y1
        return w < min_width or h < min_height

    while saved < total_count and not stop_auto_capture_flag:
        ret, frame = cap.read()
        if not ret:
            continue
        frame = cv2.flip(frame, 1)
        now = time.time()

        # Hi·ªÉn th·ªã live video
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(frame_rgb)
        display_w = 760
        display_h = int(display_w * pil_img.height / pil_img.width)
        pil_img = pil_img.resize((display_w, display_h))
        ctk_img = ctk.CTkImage(light_image=pil_img, size=pil_img.size)
        video_label.configure(image=ctk_img)
        video_label.image = ctk_img

        if now - last_time >= interval_sec:
            try:
                results = yolo_model(frame)
                boxes = results[0].boxes
            except Exception as e:
                boxes = []
                print("[WARN] YOLO error during capture:", e)

            if boxes is None or len(boxes) == 0:
                log("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t l√∫c ch·ª•p, th·ª≠ l·∫°i sau.")
                last_time = now
                time.sleep(0.2)
                continue

            box = boxes[0]
            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
            h, w = frame.shape[:2]
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(w, x2), min(h, y2)
            face = frame[y1:y2, x1:x2]

            # ------------------ CHECK QUALITY ------------------
            if face.size == 0:
                last_time = now
                continue
            if is_blurry(face):
                log("‚ö†Ô∏è ·∫¢nh m·ªù, b·ªè qua")
                last_time = now
                continue
            if is_too_dark(face):
                log("‚ö†Ô∏è ·∫¢nh t·ªëi, b·ªè qua")
                last_time = now
                continue
            if is_face_too_small((x1, y1, x2, y2)):
                log("‚ö†Ô∏è Khu√¥n m·∫∑t qu√° nh·ªè, b·ªè qua")
                last_time = now
                continue

            # ------------------ L∆ØU ·∫¢NH ------------------
            fname = os.path.join(person_dir, f"{person_id}_{int(time.time())}_{saved}.jpg")
            cv2.imwrite(fname, face)
            saved += 1
            last_time = now
            progress.set(saved / total_count)
            log(f"üíæ ƒê√£ l∆∞u {saved}/{total_count}: {fname}")

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    progress.set(0)

    if stop_auto_capture_flag:
        log("‚è∏Ô∏è Ch·ª•p t·ª± ƒë·ªông ƒë√£ b·ªã h·ªßy.")
        return

    if saved >= total_count:
        log(f"‚úÖ Ho√†n t·∫•t ch·ª•p {saved} ·∫£nh cho '{display_name}'. B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...")
        threading.Thread(target=train_single_person, args=(person_id, display_name), daemon=True).start()
    else:
        log("‚ö†Ô∏è Kh√¥ng ƒë·ªß ·∫£nh ƒë∆∞·ª£c l∆∞u, h·ªßy auto-capture.")


# ---------------- Train single person ----------------
def train_single_person(person_id, display_name):
    """
    - person_id: MSSV_T√™n (folder & embeddings)
    - display_name: t√™n hi·ªÉn th·ªã/log
    """
    global known_embeddings
    person_path = os.path.join(KNOWN_FACES_DIR, person_id)
    if not os.path.isdir(person_path):
        log(f"‚ùå Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {person_path}")
        return

    emb_list = []
    for fname in os.listdir(person_path):
        if not fname.lower().endswith((".jpg", ".jpeg", ".png")):
            continue
        p = os.path.join(person_path, fname)
        img = cv2.imread(p)
        if img is None:
            continue
        try:
            results = yolo_model(img)
            boxes = results[0].boxes
        except Exception as e:
            boxes = []

        if boxes is None or len(boxes) == 0:
            continue

        for box in boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
            face = img[y1:y2, x1:x2]
            if face.size == 0:
                continue

            try:
                aligned = align_face_mediapipe(img, (x1, y1, x2, y2), output_size=160)
            except Exception:
                aligned = None
            if aligned is None:
                aligned = center_crop_resize(img, (x1, y1, x2, y2), output_size=160)
            if aligned is None:
                continue

            face_tensor = torch.tensor(aligned.transpose(2,0,1)).unsqueeze(0).float() / 255.0
            with torch.no_grad():
                face_tensor = face_tensor.to(device)
                emb = resnet(face_tensor).detach().cpu().numpy().flatten()
            emb_list.append(emb)

    if emb_list:
        known_embeddings[person_id] = np.mean(emb_list, axis=0)
        save_known_embeddings(known_embeddings)
        log(f"üéØ ƒê√£ hu·∫•n luy·ªán & c·∫≠p nh·∫≠t embedding cho: {display_name} ({person_id})")
        messagebox.showinfo("Th√†nh c√¥ng", f"Ng∆∞·ªùi '{display_name}' ƒë√£ ƒë∆∞·ª£c th√™m v√†o h·ªá th·ªëng.")
    else:
        log(f"‚ö†Ô∏è Kh√¥ng t·∫°o ƒë∆∞·ª£c embedding cho {display_name}. Ki·ªÉm tra ·∫£nh trong {person_path}.")


def train_all_embeddings():
    log("üîß B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán to√†n b·ªô embeddings t·ª´ known_faces/ ...")
    
    #  C·∫¨P NH·∫¨T TR·∫†NG TH√ÅI KH·ªûI T·∫†O 
    app.after(0, lambda: lbl_train_status.configure(text="ƒêang x·ª≠ l√Ω: Kh·ªüi t·∫°o..."))
    
    persons = [d for d in os.listdir(KNOWN_FACES_DIR) if os.path.isdir(os.path.join(KNOWN_FACES_DIR, d))]
    total = len(persons)
    i = 0
    new_embeddings = {}
    for person in persons:
        i += 1
        #  C·∫¨P NH·∫¨T T√äN NG∆Ø·ªúI ƒêANG X·ª¨ L√ù 
        # S·ª≠ d·ª•ng lambda v·ªõi tham s·ªë m·∫∑c ƒë·ªãnh ƒë·ªÉ tr√°nh v·∫•n ƒë·ªÅ closure trong Python
        app.after(0, lambda p=person: lbl_train_status.configure(text=f"ƒêang x·ª≠ l√Ω: {p} ({i}/{total})"))
        
        # reuse train_single_person logic per-person (but to avoid repeated saving/IO, inline similar ops)
        person_path = os.path.join(KNOWN_FACES_DIR, person)
        emb_list = []
        for fname in os.listdir(person_path):
            if not fname.lower().endswith((".jpg", ".jpeg", ".png")):
                continue
            p = os.path.join(person_path, fname)
            img = cv2.imread(p)
            if img is None:
                continue
            try:
                results = yolo_model(img)
                boxes = results[0].boxes
            except Exception as e:
                boxes = []
            if boxes is None or len(boxes) == 0:
                continue
            for box in boxes:
                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
                face = img[y1:y2, x1:x2]
                if face.size == 0:
                    continue
                # Align using MediaPipe first, fallback to center-crop
                try:
                    aligned = align_face_mediapipe(img, (x1, y1, x2, y2), output_size=160)
                except Exception:
                    aligned = None

                if aligned is None:
                    aligned = center_crop_resize(img, (x1, y1, x2, y2), output_size=160)
                if aligned is None:
                    continue

                face_tensor = torch.tensor(aligned.transpose(2, 0, 1)).unsqueeze(0).float() / 255.0
                with torch.no_grad():
                    face_tensor = face_tensor.to(device)
                    emb = resnet(face_tensor).detach().cpu().numpy().flatten()
                emb_list.append(emb)
        if emb_list:
            new_embeddings[person] = np.mean(emb_list, axis=0)
            log(f"‚úÖ Hu·∫•n luy·ªán: {person} ({len(emb_list)} ·∫£nh)")
        else:
            log(f"‚ö†Ô∏è Kh√¥ng c√≥ ·∫£nh h·ª£p l·ªá cho: {person}")
        progress.set(i/max(1,total))
    # save
    save_known_embeddings(new_embeddings)
    global known_embeddings
    known_embeddings = new_embeddings
    
    #  X√ìA TR·∫†NG TH√ÅT SAU KHI HO√ÄN T·∫§T 
    app.after(0, lambda: lbl_train_status.configure(text="")) 
    
    progress.set(0)
    log("üéØ Hu·∫•n luy·ªán to√†n b·ªô ho√†n t·∫•t.")

# ---------------- UI Actions ----------------
def delete_face():
    name = simpledialog.askstring("X√≥a khu√¥n m·∫∑t", "Nh·∫≠p t√™n ng∆∞·ªùi c·∫ßn x√≥a:")
    if not name:
        return

    person_folder = os.path.join("known_faces", name)

    if os.path.exists(person_folder):
        try:
            shutil.rmtree(person_folder)
            
            # C·∫≠p nh·∫≠t l·∫°i embeddings ngay l·∫≠p t·ª©c
            if name in known_embeddings:
                del known_embeddings[name]
                save_known_embeddings(known_embeddings)
                
            messagebox.showinfo("Th√†nh c√¥ng", f"ƒê√£ x√≥a to√†n b·ªô d·ªØ li·ªáu khu√¥n m·∫∑t c·ªßa {name}.")
            print(f"[INFO] ƒê√£ x√≥a th∆∞ m·ª•c: {person_folder}")
        except Exception as e:
            messagebox.showerror("L·ªói", f"Kh√¥ng th·ªÉ x√≥a: {e}")
    else:
        messagebox.showwarning("Kh√¥ng t√¨m th·∫•y", f"Kh√¥ng t√¨m th·∫•y ng∆∞·ªùi t√™n {name} trong known_faces.")

def start_recognition_action():
    start_recognition()

def stop_recognition_action():
    stop_recognition()


def reload_embeddings_action():
    threading.Thread(target=train_all_embeddings, daemon=True).start()

def show_stats_action():
    """
    Hi·ªÉn th·ªã c·ª≠a s·ªï th·ªëng k√™ chi ti·∫øt cho ng√†y h√¥m nay.
    - T√≠nh to√°n th·ªùi gian l√†m vi·ªác (Earliest V√†o ƒë·∫øn Latest Ra).
    - Hi·ªÉn th·ªã tr·∫°ng th√°i (Ho√†n t·∫•t/ƒê√£ v√†o/Ch∆∞a v√†o).
    """
    if not os.path.exists(ATTENDANCE_FILE):
        messagebox.showinfo("Th√¥ng b√°o", "Ch∆∞a c√≥ d·ªØ li·ªáu ƒëi·ªÉm danh.")
        return
    try:
        df = pd.read_csv(ATTENDANCE_FILE)
        if "Time" not in df.columns or "Name" not in df.columns or "Type" not in df.columns:
            messagebox.showerror("L·ªói", "File ƒëi·ªÉm danh b·ªã thi·∫øu c·ªôt c·∫ßn thi·∫øt (Name, Time, Type).")
            return
    except Exception as e:
        messagebox.showerror("L·ªói", f"Kh√¥ng th·ªÉ ƒë·ªçc file ƒëi·ªÉm danh: {e}")
        return

    today = datetime.now().strftime("%Y-%m-%d")
    # L·ªçc d·ªØ li·ªáu h√¥m nay v√† t·∫°o b·∫£n sao ƒë·ªÉ tr√°nh SettingWithCopyWarning
    df_today = df[df["Time"].astype(str).str.startswith(today)].copy()

    if df_today.empty:
        # show in simple window (Top Level)
        top = ctk.CTkToplevel(app)
        top.geometry("700x500")
        top.title("üìä Th·ªëng k√™ ƒëi·ªÉm danh h√¥m nay")
        txt = ctk.CTkTextbox(master=top, width=660, height=420)
        txt.pack(padx=10, pady=10)
        txt.insert("0.0", "Kh√¥ng c√≥ d·ªØ li·ªáu ƒëi·ªÉm danh h√¥m nay.")
        txt.configure(state="disabled")
        return

    # Convert Time column to datetime objects
    df_today.loc[:, 'Time'] = pd.to_datetime(df_today['Time'])

    summary = []
    
    # Aggregate data by person
    for name, group in df_today.groupby('Name'):
        logs_in = group[group['Type'] == 'V√†o']
        logs_out = group[group['Type'] == 'Ra']

        first_in = logs_in['Time'].min() if not logs_in.empty else None
        last_out = logs_out['Time'].max() if not logs_out.empty else None
        
        working_time_str = "N/A"
        status = "‚ùå Ch∆∞a v√†o"

        if first_in:
            if last_out and last_out > first_in:
                # Calculate working time (timedelta)
                time_delta = last_out - first_in
                
                # Format timedelta to HH:MM:SS
                total_seconds = int(time_delta.total_seconds())
                hours = total_seconds // 3600
                minutes = (total_seconds % 3600) // 60
                seconds = total_seconds % 60
                
                working_time_str = f"{hours:02d}h {minutes:02d}m {seconds:02d}s"
                status = "‚úÖ Ho√†n t·∫•t (V√†o/Ra)"
            else:
                status = "üü† ƒê√£ v√†o (Ch∆∞a ra)"
        
        # Format times for display
        time_in_str = first_in.strftime("%H:%M:%S") if first_in else "N/A"
        time_out_str = last_out.strftime("%H:%M:%S") if last_out else "N/A"

        summary.append({
            "T√™n": name,
            "V√†o (Earliest)": time_in_str,
            "Ra (Latest)": time_out_str,
            "Th·ªùi gian ": working_time_str,
            "Tr·∫°ng th√°i": status
        })

    # Create a Pandas DataFrame for better formatting
    df_summary = pd.DataFrame(summary)

    # Prepare display text
    header = "üìä TH·ªêNG K√ä ƒêI·ªÇM DANH H√îM NAY\n"
    header += f"Ng√†y: {today}\n"
    
    # Use to_string for nice alignment
    report_text = df_summary.to_string(index=False)
    final_text = header + "\n" + report_text

    # Show the results
    top = ctk.CTkToplevel(app)
    top.geometry("800x600") # Increase size for better view
    top.title("üìä Th·ªëng k√™ ƒëi·ªÉm danh h√¥m nay")
    # S·ª≠ d·ª•ng font Courier ƒë·ªÉ ƒë·∫£m b·∫£o c√°c c·ªôt ƒë∆∞·ª£c cƒÉn ch·ªânh ƒë·ªÅu
    txt = ctk.CTkTextbox(master=top, width=780, height=520, font=("Courier", 12)) 
    txt.pack(padx=10, pady=10)
    txt.insert("0.0", final_text)
    txt.configure(state="disabled")

# Buttons binding
btn_start.configure(command=start_recognition_action)
btn_stop.configure(command=stop_recognition_action)
btn_auto_capture.configure(command=auto_capture_action)
btn_reload.configure(command=reload_embeddings_action)
btn_stats.configure(command=show_stats_action)
btn_export.configure(command=export_attendance_to_excel)
btn_declec.configure(command=delete_face)
btn_exit.configure(command=lambda: (stop_recognition(), app.destroy()))

# ---------------- Start app ----------------
if __name__ == "__main__":
    log("·ª®ng d·ª•ng s·∫µn s√†ng. Nh·∫≠p t√™n, ƒëi·ªÅu ch·ªânh th·ªùi gian v√† s·ªë ·∫£nh, b·∫•m 'Ch·ª•p t·ª± ƒë·ªôngs' ho·∫∑c 'B·∫Øt ƒë·∫ßu nh·∫≠n di·ªán'.")
    app.mainloop()  